---
layout: paper
id: mirage
categories: papers
permalink: papers/mirage
title: "MIRAGE: Multi-model Interface for Reviewing and Auditing Generative Text-to-Image AI"
authors:
  - Matheus Kunzler Maldaner
  - Wesley Hanwen Deng
  - Jason I. Hong
  - Ken Holstein
  - Motahhare Eslami
venue: Conference on Human Computation and Crowdsourcing (Works-in-Progress)
venue-shorthand: HCOMP
year: 2024
url: /papers/mirage
pdf: https://www.humancomputation.com/assets/wip_2024/HCOMP_24_WIP_4.pdf
link: https://www.humancomputation.com/assets/wip_2024/HCOMP_24_WIP_4.pdf
selected: true
type: workshop
figure: /images/papers/paper_mirage.png
coming-soon: false

bibtex: |-

  @inproceedings{maldaner2024mirage,
    title={MIRAGE: Multi-model Interface for Reviewing and Auditing Generative Text-to-Image AI},
    author={Maldaner, Matheus Kunzler and Deng, Wesley Hanwen and Hong, Jason I. and Holstein, Ken and Eslami, Motahhare},
    booktitle={Proceedings of the ACM Conference on Human Computation and Crowdsourcing (Works-in-Progress)},
    year={2024}
  }
---

MIRAGE is a web-based auditing workflow that allows participants to contrast outputs from multiple text-to-image (T2I) models on a single canvas. By centering lived experience and supporting side-by-side comparison, MIRAGE helped participants uncover subtle biases in generative models during a preliminary study. The interface offers a structured path for non-AI experts to document and submit model feedback that can inform safer, more inclusive generative AI systems.
